# Project with SPARK.

### The purpose.

The purpose is to create an ETL that allows monitoring and automating the loading in the data warehouse with the airflow.

### Data.

***Song datasets***: This set contains the musics and artists. A sample of this files is:
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

***Log datasets***: contains the events generated by the users. A sample of a single row of each files is:
```
{"artist":"Sydney Youngblood","auth":"Logged In","firstName":"Jacob","gender":"M","itemInSession":53,"lastName":"Klein","length":238.07955,"level":"paid","location":"Tampa-St. Petersburg-Clearwater, FL","method":"PUT","page":"NextSong","registration":1540558108796.0,"sessionId":954,"song":"Ain't No Sunshine","status":200,"ts":1543449657796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.78.2 (KHTML, like Gecko) Version\/7.0.6 Safari\/537.78.2\"","userId":"73"}
```

## Project structure

This structure that is described is in the airflow directory:

* ***dags***: Directory containing the dags.

* ***dags/etl.py***: Contains the operators for the ETL.

* ***plugins/operators***: Contains custom operators.
	* ***stage_redshift.py***: Defines the StageToRedshiftOperator operator that is responsible for copying the JSON data from the udacity s3 bucket to the redshift cluster stage table
    * ***load_dimension.py***: Defines the LoadDimensionOperator operator. It is responsible for reading the stage table and generating the dimensions.
    * ***load_fact.py***: Defines the LoadFactOperator operator is responsible for reading the stage table and generating the facts table
    * ***data_quality.py***: Defines the operator DataQualityOperator is responsible for validating if the data was inserted correctly.
* ***helpers/sql_queries***: Contains the SQL queries.

* ***README.md***: This file contains the description of the project.

### Prerequisites
* Airflow.
* Aws credentials.

### Installing.

1. Create a cluster in redshift and run **create_tables.sql** only once.

2. Register the aws credentials in Airflow connections with the name **aws_credentials**

3. Register the redshift credentials in Airflow connections with the name **redshift**

## Running the tests

Validate that the DAG terminates successfully.

## Authors

* **Jose Marquez** - [Github](https://github.com/jmarquez42)

